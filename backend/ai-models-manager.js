// Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ù†Ù…Ø§Ø°Ø¬ AI Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…ÙØªÙˆØ­Ø©
class AIModelsManager {
  constructor() {
    this.models = this.initializeModels();
    this.activeModel = null;
    this.fallbackChain = [];
    this.performance = {};
    this.healthCheck = {};
  }

  initializeModels() {
    return {
      // Ù†Ù…Ø§Ø°Ø¬ Ù…ÙØªÙˆØ­Ø© Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
      ollama: {
        name: 'Ollama (Local)',
        type: 'local',
        endpoint: 'http://localhost:11434',
        models: ['llama2', 'mistral', 'neural-chat'],
        performance: 0,
        status: 'checking',
        cost: 0
      },
      huggingface: {
        name: 'Hugging Face',
        type: 'api',
        endpoint: 'https://api-inference.huggingface.co/models/',
        models: ['meta-llama/Llama-2-7b', 'mistralai/Mistral-7B', 'bigcode/starcoder'],
        performance: 0,
        status: 'checking',
        cost: 0
      },
      together: {
        name: 'Together AI',
        type: 'api',
        endpoint: 'https://api.together.xyz',
        models: ['meta-llama/Llama-2-70b', 'mistralai/Mixtral-8x7B', 'NousResearch/Nous-Hermes-2-Mixtral-8x7B'],
        performance: 0,
        status: 'checking',
        cost: 0
      },
      replicate: {
        name: 'Replicate',
        type: 'api',
        endpoint: 'https://api.replicate.com',
        models: ['llama2', 'mistral', 'openhermes'],
        performance: 0,
        status: 'checking',
        cost: 0.0005
      },
      local_ollama: {
        name: 'Local Ollama Advanced',
        type: 'local',
        endpoint: 'http://localhost:11434',
        models: ['neural-chat:7b', 'dolphin-mixtral', 'orca-mini'],
        performance: 0,
        status: 'checking',
        cost: 0
      },
      firebase_genai: {
        name: 'Firebase Gen AI',
        type: 'api',
        endpoint: 'https://generativelanguage.googleapis.com',
        models: ['gemini-pro', 'gemini-1.5-pro'],
        performance: 0,
        status: 'checking',
        cost: 0
      }
    };
  }

  // ÙØ­Øµ ØµØ­Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
  async checkAllHealth() {
    const results = {};
    
    for (const [key, model] of Object.entries(this.models)) {
      results[key] = await this.checkModelHealth(key, model);
    }

    // ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø£Ø¯Ø§Ø¡
    this.fallbackChain = Object.keys(results)
      .sort((a, b) => results[b].score - results[a].score)
      .filter(k => results[k].available);

    this.activeModel = this.fallbackChain[0];
    return results;
  }

  // ÙØ­Øµ ØµØ­Ø© Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ø­Ø¯
  async checkModelHealth(key, model) {
    try {
      // Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„
      if (model.type === 'local') {
        return await this.testLocalModel(model);
      } else {
        return await this.testAPIModel(model);
      }
    } catch (err) {
      return {
        available: false,
        score: 0,
        error: err.message,
        timestamp: new Date()
      };
    }
  }

  testLocalModel(model) {
    return {
      available: true,
      score: 100,
      latency: 10,
      cost: 0,
      timestamp: new Date()
    };
  }

  testAPIModel(model) {
    // Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ø®ØªØ¨Ø§Ø± API
    return {
      available: true,
      score: 85,
      latency: 50,
      cost: model.cost || 0,
      timestamp: new Date()
    };
  }

  // Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙØ¶Ù„
  getBestModel() {
    if (!this.fallbackChain.length) {
      return null;
    }
    return this.models[this.fallbackChain[0]];
  }

  // Ø§Ù„ØªØ¨Ø¯ÙŠÙ„ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù†Ø¯ Ø§Ù„ÙØ´Ù„
  switchToNextModel(failedModel) {
    const currentIndex = this.fallbackChain.indexOf(failedModel);
    if (currentIndex !== -1 && currentIndex < this.fallbackChain.length - 1) {
      this.activeModel = this.fallbackChain[currentIndex + 1];
      console.log(`ğŸ”„ ØªÙ… Ø§Ù„ØªØ¨Ø¯ÙŠÙ„ Ø¥Ù„Ù‰: ${this.models[this.activeModel].name}`);
      return this.models[this.activeModel];
    }
    return null;
  }

  // Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨ Ù…Ø¹ Ø®ÙŠØ§Ø±Ø§Øª Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
  async process(prompt, options = {}) {
    let lastError = null;
    let attempts = 0;
    const maxAttempts = this.fallbackChain.length;

    while (attempts < maxAttempts) {
      try {
        const model = this.models[this.fallbackChain[attempts]];
        
        const result = await this.callModel(model, prompt, options);
        
        // ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£Ø¯Ø§Ø¡
        this.updatePerformance(this.fallbackChain[attempts], true);
        
        return {
          success: true,
          model: model.name,
          response: result,
          attempts: attempts + 1
        };
      } catch (err) {
        lastError = err;
        attempts++;
        this.updatePerformance(this.fallbackChain[attempts - 1], false);
        console.warn(`âš ï¸ ÙØ´Ù„ Ù…Ø­Ø§ÙˆÙ„Ø© ${attempts}: ${err.message}`);
      }
    }

    return {
      success: false,
      error: lastError?.message,
      attempts,
      fallback: 'Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø¢Ù„ÙŠØ© ÙØ´Ù„Øª'
    };
  }

  callModel(model, prompt, options) {
    // Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    return new Promise((resolve, reject) => {
      setTimeout(() => {
        if (Math.random() > 0.1) {
          resolve({
            text: `Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù…Ù† ${model.name}: ${prompt.substring(0, 50)}...`,
            tokens: Math.floor(Math.random() * 500),
            timestamp: new Date()
          });
        } else {
          reject(new Error('Network timeout'));
        }
      }, 100);
    });
  }

  updatePerformance(modelKey, success) {
    if (!this.performance[modelKey]) {
      this.performance[modelKey] = { success: 0, fail: 0, rate: 0 };
    }

    if (success) {
      this.performance[modelKey].success++;
    } else {
      this.performance[modelKey].fail++;
    }

    const total = this.performance[modelKey].success + this.performance[modelKey].fail;
    this.performance[modelKey].rate = (this.performance[modelKey].success / total) * 100;
  }

  // ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡
  getPerformanceReport() {
    return {
      activeModel: this.activeModel,
      fallbackChain: this.fallbackChain,
      performance: this.performance,
      availableModels: this.fallbackChain.length,
      totalModels: Object.keys(this.models).length
    };
  }

  // Ø¥Ø¶Ø§ÙØ© Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø¯ÙŠØ¯
  addCustomModel(key, modelConfig) {
    this.models[key] = {
      ...modelConfig,
      performance: 0,
      status: 'new'
    };
    return true;
  }
}

module.exports = new AIModelsManager();
